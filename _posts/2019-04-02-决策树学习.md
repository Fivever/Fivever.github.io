---
layout: post
title: 决策树学习
date: 2019-04-02
categories: 模式识别与机器学习
tag: 读书笔记
---

* content
{:toc}

统计学，数据挖掘和机器学习中的决策树训练，使用决策树作为预测模型来预测样本的类标。这种决策树也称作分类树或回归树。在这些树的结构里， 叶子节点给出类标而内部节点代表某个属性。

介绍决策树之前先介绍信息论的相关知识。

1. 随机事件的自信息量定义为该事件发生概率的对数的负值。设事件x发生的概率为p(x)，则它的自信息量定义为

    <img src="https://latex.codecogs.com/png.latex?I(x)=-log p(x)">

2. 信息熵H(x)是随机变量X的概率分布的函数，所以也称为熵函数。如果把概率分布p(x),i=1,2,...,q，记为<img src="https://latex.codecogs.com/png.latex?p_1,p_2,...,p_q">，则熵函数又可以写成概率矢量<img src="https://latex.codecogs.com/png.latex?p=(p_1,p_2,...,p_q)">的函数形式，记为H(p)。

    <img src="https://latex.codecogs.com/png.latex?H(X)=-\sum_{i=1}^qp_ilogp_i=H(p_1,...,p_q)=H(p)">

3. 一个事件y所给出关于另一个事件x的信息定义为互信息（信息增益），用I(x;y)表示。

    <img src="https://latex.codecogs.com/png.latex?I(x;y)=\sum_{i=1}^n\sum_{j=1}^mp(x_iy_j)I(x_i;y_i)">

    互信息I(X;Y)是已知事件y后所消除的关于x的不确定性，它等于事件x本身的不确定性H(X)减去已知事件Y后对X仍然存在的不确定性<img src="https://latex.codecogs.com/png.latex?H(X\mid Y)">即<img src="https://latex.codecogs.com/png.latex?I(X;Y)=H(X)-H(X\mid Y)">

4. Gini 指数
基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。

注意： Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。

即 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率

<img src="https://latex.codecogs.com/png.latex?Gini(p)=\sum^n_{k=1}\sum_{k'}p_ip_{k'}=1-\sum_{k=1}^np_k^2">

现在假设一个数据集

ID|A|B|C|D|tag
1|0|1|0|1|1
2|0|1|0|2|1
3|0|2|1|3|2
4|0|3|1|0|2
5|1|0|1|0|3
6|1|0|1|0|3
7|1|0|1|0|4
8|1|0|1|0|4

其中ID为数据集标号，A、B、C、D分别为该数据集的四个特征，tag为数据集的标签，我们希望训练得出决策树，用以对未来的数据进行分类。

### 特征选择

特征选择是决定用哪个特征来划分特征空间。现在存在四个可能的决策树的根结点A、B、C、D，那么问题是选择哪一个作为根特征更好一些？需要通过信息增益来解决。

这里信息增益定义为训练数据集中的类别和特征的互信息。决策树学习应用信息增益准则选择特征，给定训练数据集X和特征A，H(X)表示对X分类的不确定性，而<img src="https://latex.codecogs.com/png.latex?H(X\mid A)">表示在特征A给定的条件下对数据集X进行分类的不确定性，差值为信息增益。不同的特征会有不同的信息增益，比较大小后信息增益大的特征具有更强的分类能力。

1. 首先计算类别的信息熵H(X)

    <img src="https://latex.codecogs.com/png.latex?H(X)=I(X=1)+I(X=2)+I(X=3)+I(X=4)">

2. 计算四个特征的信息增益

    <img src="https://latex.codecogs.com/png.latex?I(X;A)=H(X)-H(X\mid A)\\=H(X)-I(X=1,A=0)-I(X=1,A=0)-I(X=2,A=0)-I(X=2,A=0)-I(X=3,A=1)-I(X=3,A=1)-I(X=4,A=1)-I(X=4,A=1)">

    其余同理，信息增益最大，则为最优特征。

### 决策树的生成

<img src="https://user-gold-cdn.xitu.io/2018/3/11/162149725abc5fcb?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">

决策树的各个节点上应用信息增益准则选择特征，递归构建决策树，具体方法是从根节点开始，对节点计算所以可能的特征信息增益，选择信息增益最大的特征作为节点的特征，在对子节点递归调用以上方法构建决策树。

- CEntropy.m
  ```matlab
  % 计算类别的信息熵
  function result=CEntropy(propertyList)
  result=0;
  totalLength=length(propertyList);
  temList=unique(propertyList);
  pNum=length(itemList);
  for i=1:pNum
      itemLength=length(find(propertyList==itemList(i)));
      pItem=itemLength/totalLength;
  result=result-pItem*log2(pItem);
  end
  ```
- getFeatureNum.m
  ```matlab
  function result=GetFeatureNum(propertyName,str)
  result=0;
  for i=1:length(propertyName)
      if strcmp(propertyName{i},str)==1
          result=i;
          break;
      end
  end
  ```
- CalculateNode
  ```matlab
  % 判断是否建立子节点
  % 得到所建立的子节点的属性

  function [NodeIndex,BuildNode]=CalculateNode(data,label,delta)
  LargeEntropy=CEntropy(label);
  [m,n]=size(data);   % m是样本数，n是属性个数
  EntropyGain=LargeEntropy*ones(1,n);
  BuildNode=true;
  for i=1:n
      pData=data(:,i);
      itemList=unique(pData);
      for j=1:length(itemList)
          itemIndex=find(pData==itemList(j));
          EntropyGain(i)=EntropyGain(i)-length(itemIndex)/m*CEntropy(label(itemIndex));   % 计算信息增益=原信息熵+增加属性的信息熵
      end
    % EntropyGain(i)=EntropyGain(i)/CEntropy(pData); 
  end
  [maxGainEntropy,NodeIndex]=max(EntropyGain);
  if maxGainEntropy < delta % 信息增益小于阈值则不新建节点
      BuildNode=false;
  end
  ```
- BuildTree.m
  ```matlab
  % 建立子节点
  function BuildTree(fatherlevel,fatherNodeName,edge,data,label,propertyName,delta)
  global Node;
  sonNode=struct('level',0,'fatherNodeName',[],'EdgeProperty',[],'NodeName',[]);
  sonNode.level=fatherlevel+1;
  sonNode.fatherNodeName=fatherNodeName;
  sonNode.EdgeProperty=edge;
  if length(unique(label))==1 % 都是同一类则不再分类
      sonNode.NodeName=label(1);
      Node=[Node sonNode];
      return;
  end
  if length(propertyName)<1   % 没有可分的属性
      labelSet=unique(label);
      k=length(labelSet);
      labelNum=zeros(k,1);
      for i=1:k
          labelNum(i)=length(find(label==labelSet(i)));
      end
      [~,labelIndex]=max(labelNum);
      sonNode.NodeName=labelSet(labelIndex);  % 选择最多的类别作为节点名
      Node=[Node sonNode];
      return;
  end
  [sonIndex,BuildNode]=CalculateNode(data,label,delta); % 判断是否建立新节点
  if BuildNode
      dataRowIndex=setdiff(1:length(propertyName),sonIndex);
      sonNode.NodeName=propertyName{sonIndex};
      Node=[Node sonNode];
      propertyName(sonIndex)=[];  % 删除该类
      sonData=data(:,sonIndex);
      sonEdge=unique(sonData);
    
      for i=1:length(sonEdge)
          edgeDataIndex=find(sonData==sonEdge(i));    % 递归调用建树
          BuildTree(sonNode.level,sonNode.NodeName,sonEdge(i),data(edgeDataIndex,dataRowIndex),label(edgeDataIndex,:),propertyName,delta);
      end
  else    % 同上，选择最多的类别作为节点名
      labelSet=unique(label);
      k=length(labelSet);
      labelNum=zeros(k,1);
      for i=1:k
          labelNum(i)=length(find(label==labelSet(i)));
      end
      [~,labelIndex]=max(labelNum);
      sonNode.NodeName=labelSet(labelIndex);
      Node=[Node sonNode];
      return;
  end
  ```
- decisionTree.m
    ```matlab
    function decisionTreeModel=decisionTree(data,label,propertyName,delta)
    global Node;
    
    Node=struct('level',-1,'fatherNodeName',[],'EdgeProperty',[],'NodeName',[]);
    BuildTree(-1,'root','Stem',data,label,propertyName,delta);
    Node(1)=[];
    model.Node=Node;
    decisionTreeModel=model;

    for i=1:length(Node)    % 显示节点属性和树的信息
        disp(Node(i));
    end
    ```
- main.m
  ```matlab
  data = load('data.txt') % 均为属性类别，m列个不同属性，n行个样本
  Alt = data(:,1);    % 以下均为样本属性
  Bar = data(:,2);
  Fri = data(:,3);
  Hun = data(:,4);
  Pat = data(:,5);
  Price = data(:,6);
  Rain = data(:,7);
  Res = data(:,8);
  Type = data(:,9);
  Est= data(:,10);
  goal = [1;0;1;1;0;1;0;1;0;0;0;1];   % 样本类别
  propertyName={ 'Alt'; 'Bar'; 'Fri'; 'Hun'; 'Pat'; 'Price'; 'Rain'; 'Res'; 'Type'; 'Est'};   % 属性名称
  propertyName=propertyName';
  delta=0;
  decisionTreeModel=decisionTree(data,goal,propertyName,delta);
  ```