---
layout: post
title: 决策树学习
date: 2019-04-02
categories: 模式识别与机器学习
tag: 读书笔记
---

统计学，数据挖掘和机器学习中的决策树训练，使用决策树作为预测模型来预测样本的类标。这种决策树也称作分类树或回归树。在这些树的结构里， 叶子节点给出类标而内部节点代表某个属性。

介绍决策树之前先介绍信息论的相关知识。

1. 随机事件的自信息量定义为该事件发生概率的对数的负值。设事件x发生的概率为p(x)，则它的自信息量定义为

    <img src="https://latex.codecogs.com/png.latex?I(x)=-log p(x)">

2. 一个事件y所给出关于另一个事件x的信息定义为互信息，用I(x;y)表示。

    <img src="https://latex.codecogs.com/png.latex?I(x;y)=I(x)-I(x\mid y)=log\frac{p(x\mid y)}{p(x)}=I(y;x)">

    互信息I(x;y)是已知事件y后所消除的关于x的不确定性，它等于事件x本身的不确定性I(x)减去已知事件y后对x仍然存在的不确定性<img src="https://latex.codecogs.com/png.latex?I(x\mid y)">

3. 信息熵H(x)是随机变量X的概率分布的函数，所以也称为熵函数。如果把概率分布p(x),i=1,2,...,q，记为<img src="https://latex.codecogs.com/png.latex?p_1,p_2,...,p_q">，则熵函数又可以写成概率矢量<img src="https://latex.codecogs.com/png.latex?p=(p_1,p_2,...,p_q)">的函数形式，记为H(p)。

    <img src="https://latex.codecogs.com/png.latex?H(X)=-\sum_{i=1}^qp_ilogp_i=H(p_1,...,p_q)=H(p)">

现在假设一个数据集


ID|A|B|C|D|tag
1|0|1|0|1|1
2|0|1|0|2|1
3|0|2|1|3|2
4|0|3|1|0|2
5|1|0|1|0|3
6|1|0|1|0|3
7|1|0|1|0|4
8|1|0|1|0|4

其中ID为数据集标号，A、B、C、D分别为该数据集的四个特征，tag为数据集的标签，我们希望训练得出决策树，用以对未来的数据进行分类。

### 特征选择

特征选择是决定用哪个特征来划分特征空间。现在存在四个可能的决策树的根结点A、B、C、D，那么问题是选择哪一个作为根特征更好一些？需要通过信息增益来解决。

这里信息增益定义为训练数据集中的类别和特征的互信息。决策树学习应用信息增益准则选择特征，给定训练数据集X和特征A，H(X)表示对X分类的不确定性，而<img src="https://latex.codecogs.com/png.latex?H(X\mid A)">表示在特征A给定的条件下对数据集X进行分类的不确定性，差值为信息增益。不同的特征会有不同的信息增益，比较大小后信息增益大的特征具有更强的分类能力。

1. 首先计算类别的信息熵H(X)

    <img src="https://latex.codecogs.com/png.latex?H(X)=I(X=1)+I(X=2)+I(X=3)+I(X=4)">

2. 计算四个特征的信息增益

    <img src="https://latex.codecogs.com/png.latex?I(X,A)=H(X)-H(X,A)=H(X)-I(X=1,A=0)-I(X=1,A=0)-I(X=2,A=0)-I(X=2,A=0)-I(X=3,A=1)-I(X=3,A=1)-I(X=4,A=1)-I(X=4,A=1)">

    其余同理。

3. 决策树的生成

    决策树的各个节点上应用信息增益准则选择特征，递归构建决策树，具体方法是从根节点开始，对节点计算所以可能的特征信息增益，选择信息增益最大的特征作为节点的特征，在对子节点递归调用以上方法构建决策树。