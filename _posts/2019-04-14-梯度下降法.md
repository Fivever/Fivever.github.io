---
layout: post
title: 梯度下降法
date: 2019-04-14
categories: 模式识别与机器学习
tag: 读书笔记
---


## 介绍

梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

简单地来说，多元函数的“导数”(derivative)就是梯度(gradient)。

比如说，一元函数<img src="https://latex.codecogs.com/png.latex?f(x)=(x-a)^2">的导数是

<img src="https://latex.codecogs.com/png.latex?\frac{\partial f}{\partial x}=2(x-a)">

而n元函数<img src="https://latex.codecogs.com/png.latex?f(x_1,x_2,...,x_n)">的梯度是一个长度为n的向量，向量中第k个元素为函数f对变量<img src="https://latex.codecogs.com/png.latex?x_k">的偏导数。

<img src="https://latex.codecogs.com/png.latex?\bigtriangledown f(x_1,x_2,...,x_n)=(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n})">

## 梯度下降法

对于凸优化问题来说，导数为0（梯度为0向量）的点，就是优化问题的解。

为了找到这个解，我们沿着梯度的反方向进行线性搜索，每次搜索的步长为某个特定的数值α，直到梯度与0向量非常接近为止。上面描述的这个方法就是梯度下降法。迭代算法的步骤如下：

（1）当i=0，自己设置初始点<img src="https://latex.codecogs.com/png.latex?x^0=(x^0_1,x^0_2,...,x^0_n)">，设置步长（也叫做学习率）<img src="https://latex.codecogs.com/png.latex?\alpha">，设置迭代终止的误差忍耐度tol。
（2）计算目标函数f在点<img src="https://latex.codecogs.com/png.latex?x^i">上的梯度<img src="https://latex.codecogs.com/png.latex?\bigtriangledown f_{x^i}">。
（3）计算<img src="https://latex.codecogs.com/png.latex?x^{i+1}">，公式如下

<img src="https://latex.codecogs.com/png.latex?x^{i+1}=x^i−\alpha \bigtriangledown f_{x^i}">

（4）计算梯度<img src="https://latex.codecogs.com/png.latex?\bigtriangledown f_{x^{i+1}}">。如果<img src="https://latex.codecogs.com/png.latex?\mid \bigtriangledown f_{x^{i+1}}\mid^2 \leqslant tol">则迭代停止，最优解的取值为<img src="https://latex.codecogs.com/png.latex?x^{i+1}">；如果梯度的二范数大于tol，那么i=i+1，并返回第（3）步。

## 三种梯度下降法

在机器学习中，我们通常会有一个损失函数<img src="https://latex.codecogs.com/png.latex?L(\beta)">，其中向量<img src="https://latex.codecogs.com/png.latex?\beta=(\beta_0,\beta_1,...,\beta_n)">是模型中的参数，我们需要找到最优的<img src="https://latex.codecogs.com/png.latex?\beta">来最小化损失函数<img src="https://latex.codecogs.com/png.latex?L(\beta)">。所以说，模型的训练过程也就是寻找最优解的过程。

### 全批量梯度下降法

全批量梯度下降（full-batch gradient descent）是梯度下降法在机器学习中的直接应用。

``` Python
# 引用模块
import pandas as pd
import numpy as np

# 导入数据
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
submit = pd.read_csv('sample_submit.csv')

# 初始设置
beta = [1, 1]
alpha = 0.0002
tol_L = 0.00001

# 对x进行归一化
max_x = max(train['id'])
x = train['id'] / max_x
y = train['questions']

# 定义计算梯度的函数
def compute_grad(beta, x, y):
    grad = [0, 0]
    grad[0] = 2. * np.mean(beta[0] + beta[1] * x - y)
    grad[1] = 2. * np.mean(x * (beta[0] + beta[1] * x - y))
    return np.array(grad)

# 定义更新beta的函数
def update_beta(beta, alpha, grad):
    new_beta = np.array(beta) - alpha * grad
    return new_beta

# 定义计算RMSE的函数
def rmse(beta, x, y):
    squared_err = (beta[0] + beta[1] * x - y) ** 2
    res = np.sqrt(np.mean(squared_err))
    return res

# 进行第一次计算
grad = compute_grad(beta, x, y)
loss = rmse(beta, x, y)
beta = update_beta(beta, alpha, grad)
loss_new = rmse(beta, x, y)

# 开始迭代
i = 1
while np.abs(loss_new - loss) > tol_L:
    beta = update_beta(beta, alpha, grad)
    grad = compute_grad(beta, x, y)
    loss = loss_new
    loss_new = rmse(beta, x, y)
    i += 1
    print('Round %s Diff RMSE %s'%(i, abs(loss_new - loss)))
print('Coef: %s \nIntercept %s'%(beta[1], beta[0]))
```

### 随机梯度下降法

随机梯度下降法（Stochastic Gradient Decent, SGD）是对全批量梯度下降法计算效率的改进算法。本质上来说，我们预期随机梯度下降法得到的结果和全批量梯度下降法相接近；SGD的优势是更快地计算梯度。

我们先回顾以下全批量法是如何计算每次迭代中的梯度的：

<img src="https://latex.codecogs.com/png.latex?\nabla L = \left(\frac{\partial L}{\partial \beta_0}, \frac{\partial L}{\partial \beta_1}\right)=\left(\frac{2}{N}\sum_{j=1}^N(\beta_0+\beta_1x_j-\hat y_j), \frac{2}{N}\sum_{j=1}^Nx_j(\beta_0+\beta_1x_j-\hat y_j)\right)">

其中N是样本数量，我们很容易看出，对于最小二乘法来说，每计算一次梯度的代价是O(N)，运算次数与N成线性关系。

而随机梯度下降法能将计算一次梯度的代价降低到O(1)，也就是运算次数为常数次，与N无关。所以SGD特别适合大训练样本的计算。

SGD在计算<img src="https://latex.codecogs.com/png.latex?\nabla L">时，并不使用全部样本，而是随机地挑选了一个样本<img src="https://latex.codecogs.com/png.latex?(x_r,y_r)">，

<img src="https://latex.codecogs.com/png.latex?\nabla L = \left(\frac{\partial L}{\partial \beta_0}, \frac{\partial L}{\partial \beta_1}\right)=\left(2(\beta_0+\beta_1x_r-\hat y_r), 2x_r(\beta_0+\beta_1x_r-\hat y_r)\right)">

根据上面的式子，我们修改了上面使用过的compute_grad函数。新函数compute_grad_SGD(beta, x, y)会每次随机选择一个样本，并且根据这个样本计算出梯度。这就是所谓的随机梯度下降法。

在原来的第（4）步中，我们每次迭代都要计算损失函数的变化<img src="https://latex.codecogs.com/png.latex?\mid L(\beta^{i+1})−L(\beta^i)\mid">。而计算这个损失函数的代价是和全批量法中计算梯度的代价相等的。我们可以改成每进行100次迭代，检查一次<img src="https://latex.codecogs.com/png.latex?\mid L(\beta^{i+1})−L(\beta^i)\mid">。

```Python
# 引用模块
import pandas as pd
import numpy as np

# 导入数据
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
submit = pd.read_csv('sample_submit.csv')

# 初始设置
beta = [1, 1]
alpha = 0.2
tol_L = 0.1

# 对x进行归一化
max_x = max(train['id'])
x = train['id'] / max_x
y = train['questions']

# 定义计算随机梯度的函数
def compute_grad_SGD(beta, x, y):
    grad = [0, 0]
    r = np.random.randint(0, len(x))
    grad[0] = 2. * np.mean(beta[0] + beta[1] * x[r] - y[r])
    grad[1] = 2. * np.mean(x[r] * (beta[0] + beta[1] * x[r] - y[r]))
    return np.array(grad)

# 定义更新beta的函数
def update_beta(beta, alpha, grad):
    new_beta = np.array(beta) - alpha * grad
    return new_beta

# 定义计算RMSE的函数
def rmse(beta, x, y):
    squared_err = (beta[0] + beta[1] * x - y) ** 2
    res = np.sqrt(np.mean(squared_err))
    return res

# 进行第一次计算
np.random.seed(10)
grad = compute_grad_SGD(beta, x, y)
loss = rmse(beta, x, y)
beta = update_beta(beta, alpha, grad)
loss_new = rmse(beta, x, y)

# 开始迭代
i = 1
while np.abs(loss_new - loss) > tol_L:
    beta = update_beta(beta, alpha, grad)
    grad = compute_grad_SGD(beta, x, y)
    if i % 100 == 0:
        loss = loss_new
        loss_new = rmse(beta, x, y)
        print('Round %s Diff RMSE %s'%(i, abs(loss_new - loss)))
    i += 1

print('Coef: %s \nIntercept %s'%(beta[1], beta[0]))
```

### 小批量随机梯度下降法

小批量随机梯度下降法（Mini-batch Stochastic Gradient Decent）是对速度和稳定性进行妥协后的产物。

小批量随机梯度下降的关键思想是，我们不是随机使用一个样本，而是随机使用b个不同的样本。梯度的计算如下：

<img src="https://latex.codecogs.com/png.latex?\nabla L = \left(\frac{\partial L}{\partial \beta_0}, \frac{\partial L}{\partial \beta_1}\right)=\left(\frac{2}{b}\sum_{r=1}^b(\beta_0+\beta_1x_{j_r}-\hat y_{j_r}), \frac{2}{b}\sum_{r=1}^bx_{j_r}(\beta_0+\beta_1x_{j_r}-\hat y_{j_r})\right)">

我们可以看出当b=1时，小批量随机下降法就等价与SGD；当b=N时，小批量就等价于全批量。所以小批量梯度下降法的效果也和b的选择相关，这个数值被称为批量尺寸(batch size)。

```Python
# 引用模块
import pandas as pd
import numpy as np

# 导入数据
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
submit = pd.read_csv('sample_submit.csv')

# 初始设置
beta = [1, 1]
alpha = 0.2
tol_L = 0.1
batch_size = 16

# 对x进行归一化
max_x = max(train['id'])
x = train['id'] / max_x
y = train['questions']

# 定义计算mini-batch随机梯度的函数
def compute_grad_batch(beta, batch_size, x, y):
    grad = [0, 0]
    r = np.random.choice(range(len(x)), batch_size, replace=False)
    grad[0] = 2. * np.mean(beta[0] + beta[1] * x[r] - y[r])
    grad[1] = 2. * np.mean(x[r] * (beta[0] + beta[1] * x[r] - y[r]))
    return np.array(grad)

# 定义更新beta的函数
def update_beta(beta, alpha, grad):
    new_beta = np.array(beta) - alpha * grad
    return new_beta

# 定义计算RMSE的函数
def rmse(beta, x, y):
    squared_err = (beta[0] + beta[1] * x - y) ** 2
    res = np.sqrt(np.mean(squared_err))
    return res

# 进行第一次计算
np.random.seed(10)
grad = compute_grad_batch(beta, batch_size, x, y)
loss = rmse(beta, x, y)
beta = update_beta(beta, alpha, grad)
loss_new = rmse(beta, x, y)

# 开始迭代
i = 1
while np.abs(loss_new - loss) > tol_L:
    beta = update_beta(beta, alpha, grad)
    grad = compute_grad_batch(beta, batch_size, x, y)
    if i % 100 == 0:
        loss = loss_new
        loss_new = rmse(beta, x, y)
        print('Round %s Diff RMSE %s'%(i, abs(loss_new - loss)))
    i += 1
print('Coef: %s \nIntercept %s'%(beta[1], beta[0]))
```