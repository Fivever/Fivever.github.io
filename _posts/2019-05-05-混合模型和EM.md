---
layout: post
title: 混合模型和EM
date: 2019-05-05
categories: 模式识别与机器学习
tag: 读书笔记
published: true
---

* content
{:toc}

## 高斯混合模型（GMM）

设有随机变量X，则混合高斯模型可以用下式表示：

<img src="https://latex.codecogs.com/png.latex?p(x)=\sum_{k=1}^K\pi_kN(x\mid \mu_k,\Sigma_k)">

其中<img src="https://latex.codecogs.com/png.latex?N(x\mid \mu_k,\Sigma_k)">称为混合模型中的第k个分量（component）。<img src="https://latex.codecogs.com/png.latex?\pi_k">是混合系数（mixture coefficient），且满足：

<img src="https://latex.codecogs.com/png.latex?\sum_{k=1}^K\pi_k=1">

<img src="https://latex.codecogs.com/png.latex?0\leqslant \pi_k\leqslant 1">

实际上，可以认为<img src="https://latex.codecogs.com/png.latex?\pi_k">就是每个分量<img src="https://latex.codecogs.com/png.latex?N(x\mid \mu_k,\Sigma_k)">的权重。

## GMM的应用

GMM常用于聚类。如果要从 GMM 的分布中随机地取一个点的话，实际上可以分为两步：首先随机地在这 K 个 Component 之中选一个，每个 Component 被选中的概率实际上就是它的系数<img src="https://latex.codecogs.com/png.latex?\pi_k">，选中 Component 之后，再单独地考虑从这个 Component 的分布中选取一个点就可以了--这里已经回到了普通的 Gaussian 分布，转化为已知的问题。

将GMM用于聚类时，假设数据服从混合高斯分布（Mixture Gaussian Distribution），那么只要根据数据推出 GMM 的概率分布来就可以了；然后 GMM 的 K 个 Component 实际上对应K个 cluster 。根据数据来推算概率密度通常被称作 density estimation 。特别地，当已知（或假定）概率密度函数的形式，而要估计其中的参数的过程被称作“参数估计”。

怎么根据数据自动确定<img src="https://latex.codecogs.com/png.latex?\pi_k">的值？这就是GMM参数估计的问题。要解决这个问题，可以使用EM算法。通过EM算法，我们可以迭代计算出GMM中的参数：<img src="https://latex.codecogs.com/png.latex?(\pi_k,\mu_k,\Sigma_k)">。

## GMM的贝叶斯理解

在介绍GMM参数估计之前，先改写GMM的形式，改写之后的GMM模型可以方便地使用EM估计参数。GMM的原始形式如下：

<img src="https://latex.codecogs.com/png.latex?p(x)=\sum_{k=1}^K\pi_kN(x\mid \mu_k,\Sigma_k)">

前面提到<img src="https://latex.codecogs.com/png.latex?\pi_k">可以看成是第k类被选中的概率。我们引入一个新的K维随机变量z。<img src="https://latex.codecogs.com/png.latex?z_k(1\leqslant k\leqslant K)">只能取0或1两个值；<img src="https://latex.codecogs.com/png.latex?z_k=1">表示第k类被选中，即：<img src="https://latex.codecogs.com/png.latex?p(z_k=1)=\pi_k">；如果<img src="https://latex.codecogs.com/png.latex?z_k=0">表示第k类没有被选中。更数学化一点，<img src="https://latex.codecogs.com/png.latex?z_k">要满足以下两个条件：

<img src="https://latex.codecogs.com/png.latex?z_k\in\{0,1\}">

<img src="https://latex.codecogs.com/png.latex?\sum_{k=1}^Kz_k=1">

<img src="https://latex.codecogs.com/png.latex?z_k=1">的概率就是<img src="https://latex.codecogs.com/png.latex?\pi_k">，假设<img src="https://latex.codecogs.com/png.latex?z_k">之间是独立同分布的（iid），我们可以写出z的联合概率分布形式，就是连乘：

<img src="https://latex.codecogs.com/png.latex?p(z)=p(z_1)p(z_2)...p(z_K)=\prod_{k=1}^K\pi_k^{z_k}">

数据可以分为几类，显然，每一类中的数据都是服从正态分布的。这个叙述可以用条件概率来表示：

<img src="https://latex.codecogs.com/png.latex?p(x\mid z_k=1)=N(x\mid \mu_k,\Sigma_k)">

即第k类中的数据服从正态分布。进而上式有可以写成如下形式：

<img src="https://latex.codecogs.com/png.latex?p(x\mid z)=\prod_{k=1}^KN(x\mid \mu_k,\Sigma_k)^{z_k}">

上面分别给出了p(z)和<img src="https://latex.codecogs.com/png.latex?p(x\mid z)">的形式，根据条件概率公式，可以求出p(x)的形式：

<img src="https://latex.codecogs.com/png.latex?p(x)=\sum_zp(z)p(x\mid z)">

<img src="https://latex.codecogs.com/png.latex?=\sum_z(\prob_{k=1}^K\pi_k^{z_k}N(x\mid \mu_k,\Sigma_k)^{z_k})=\sum_{k=1}^K\pi_kN(x\mid \mu_k,\Sigma_k)">

可以看到GMM模型的p(x)与上式有一样的形式，且上式中引入了一个新的变量z，通常称为隐含变量（latent variable）。隐含的意义是：我们知道数据可以分成几类，但是随机抽取一个数据点，我们不知道这个数据点属于第一类还是第二类，它的归属我们观察不到，因此引入一个隐含变量z来描述这个现象。

注意到在贝叶斯的思想下，p(z)是先验概率，<img src="https://latex.codecogs.com/png.latex?p(x\mid z)">是似然概率，很自然我们会想到求出后验概率<img src="https://latex.codecogs.com/png.latex?p(z\mid x)">：

<img src="https://latex.codecogs.com/png.latex?p(z_k=1\mid x)=\frac{p(z_k=1)p(x\mid z_k=1)}{p(x,z_k=1)}=\frac{\pi_kN(x\mid \mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_jN(x\mid \mu_j,\Sigma_j)}">

在贝叶斯的观点下，<img src="https://latex.codecogs.com/png.latex?\pi_k">可视为<img src="https://latex.codecogs.com/png.latex?z_k=1">的先验概率。

上述内容改写了GMM的形式，并引入了隐含变量z和已知x后的的后验概率<img src="https://latex.codecogs.com/png.latex?p(z_k\mid x)">，这样做是为了方便使用EM算法来估计GMM的参数。

## EM算法估计GMM参数

EM算法（Expectation-Maximization algorithm）分两步，第一步先求出要估计参数的粗略值，第二步使用第一步的值最大化似然函数。因此要先求出GMM的似然函数。

假设<img src="https://latex.codecogs.com/png.latex?x=\{x_1,x_2,...,x_N\}">是所有的点，GMM模型中有三个参数需要估计，分别是<img src="https://latex.codecogs.com/png.latex?\pi">，<img src="https://latex.codecogs.com/png.latex?\mu">和<img src="https://latex.codecogs.com/png.latex?\Sigma">。将GMM式稍微改写一下：

<img src="https://latex.codecogs.com/png.latex?p(x\mid \pi,\mu,\Sigma)=\sum_{k=1}^K\pi_kN(x\mid \mu_k,\Sigma_k)">

为了估计这三个参数，需要分别求解出这三个参数的最大似然函数。先求解\mu_k的最大似然函数。样本符合iid，上式所有样本连乘得到最大似然函数，对上式取对数得到对数似然函数。

然后再对<img src="https://latex.codecogs.com/png.latex?\mu_k">求导并令导数为0即得到最大似然函数。

<img src="https://latex.codecogs.com/png.latex?0=-\sum_{n=1}^N\frac{\pi_kN(x_n\mid \mu_k,\Sigma_k)}{\sum_j\pi_jN(x_n\mid \mu_j,\Sigma_j)}\sum_k(x_n-\mu_k)">

整理得到：

<img src="https://latex.codecogs.com/png.latex?\mu_k=\sum_{n=1}^Np(z_{nk}=1\mid x)x_n">

其中：<img src="https://latex.codecogs.com/png.latex?\sum_{n=1}^Np(z_{nk}=1\mid x_n)">

N表示点的数量。<img src="https://latex.codecogs.com/png.latex?p(z_{nk}=1\mid x_n)">表示点<img src="https://latex.codecogs.com/png.latex?{x}_n">属于聚类k的后验概率。则N_k可以表示属于第k个聚类的点的数量。那么<img src="https://latex.codecogs.com/png.latex?\mu_k">表示所有点的加权平均，每个点的权值是<img src="https://latex.codecogs.com/png.latex?\sum_{n=1}^N p(z_{nk}=1\mid x_n)">，跟第k个聚类有关。

同理求<img src="https://latex.codecogs.com/png.latex?\Sigma_k">的最大似然函数，可以得到：

<img src="https://latex.codecogs.com/png.latex?\Sigma_k=\frac1{N_k}\sum_{n=1}^Np(z_{nk}=1\mid x_n)(x_n-\mu_k)(x_n-\mu_k)^T">

最后剩下<img src="https://latex.codecogs.com/png.latex?\pi_k">的最大似然函数。注意到<img src="https://latex.codecogs.com/png.latex?\pi_k">有限制条件<img src="https://latex.codecogs.com/png.latex?\sum_{k=1}^K\pi_k = 1">，因此我们需要加入拉格朗日算子：

<img src="https://latex.codecogs.com/png.latex?ln\,p(x\mid \pi,\mu,\Sigma)+\lambda(\sum_{k=1}^K\pi_k-1)">

求上式关于<img src="https://latex.codecogs.com/png.latex?\pi_k">的最大似然函数，得到：

<img src="https://latex.codecogs.com/png.latex?0=\sum_{n=1}^N\frac{\pi_kN(x_n\mid \mu_k,Sigma_k)}{\sum_j\pi_jN(x_n\mid \mu_j,\Sigma_j)}+\lambda \pi_k">

由上述公式可化简得到

<img src="https://latex.codecogs.com/png.latex?0=N_k+\lambda \pi_k">

又因为<img src="https://latex.codecogs.com/png.latex?\sum_{k=1}^K\pi_k = 1">，上式两边同时对k求和。此外<img src="https://latex.codecogs.com/png.latex?N_k">表示属于第k个聚类的点的数量。对<img src="https://latex.codecogs.com/png.latex?N_k">，从k=1到k=K求和后，就是所有点的数量N:

<img src="https://latex.codecogs.com/png.latex?0=\sum_{k=1}^KN_k+\lambda\sum_{k=1}^K\pi_k">

<img src="https://latex.codecogs.com/png.latex?0=N+\lambda">

从而可得到<img src="https://latex.codecogs.com/png.latex?\lambda = -N">，带入<img src="https://latex.codecogs.com/png.latex?0=N_k+\lambda \pi_k">，进而可以得到<img src="https://latex.codecogs.com/png.latex?\pi_k">更简洁的表达式。

<img src="https://latex.codecogs.com/png.latex?\pi_k=\frac{N_k}N">

EM算法估计GMM参数即最大化

<img src="https://latex.codecogs.com/png.latex?\mu_k=\sum_{n=1}^N\frac{\pi_kN(x\mid \mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_jN(x\mid \mu_j,\Sigma_j)}x_n">

<img src="https://latex.codecogs.com/png.latex?\Sigma_k=\frac1{N_k}\sum_{n=1}^N\frac{\pi_kN(x\mid \mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_jN(x\mid \mu_j,\Sigma_j)}(x_n-\mu_k)(x_n-\mu_k)^T">

<img src="https://latex.codecogs.com/png.latex?\pi_k=\frac{N_k}N">

我们先指定<img src="https://latex.codecogs.com/png.latex?\pi">，<img src="https://latex.codecogs.com/png.latex?\mu">和<img src="https://latex.codecogs.com/png.latex?\Sigma">的初始值，带入上式，求得<img src="https://latex.codecogs.com/png.latex?\pi_k">，<img src="https://latex.codecogs.com/png.latex?\mu_k">和<img src="https://latex.codecogs.com/png.latex?\Sigma_k">；接着用求得的<img src="https://latex.codecogs.com/png.latex?\pi_k">，<img src="https://latex.codecogs.com/png.latex?\mu_k">和<img src="https://latex.codecogs.com/png.latex?\Sigma_k">再带入上式得到新的<img src="https://latex.codecogs.com/png.latex?\pi">，<img src="https://latex.codecogs.com/png.latex?\mu">和<img src="https://latex.codecogs.com/png.latex?\Sigma">值，如此往复，直到算法收敛。