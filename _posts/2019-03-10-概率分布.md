---
layout: post
title: 概率分布
date: 2019-03-10
categories: 模式识别与机器学习
tag: 读书笔记
published: true
---

* content
{:toc}

## 二元变量

伯努利分布（英语：Bernoulli distribution，又名两点分布或者0-1分布，是一个离散型概率分布，为纪念瑞士科学家雅各布·伯努利而命名。）若伯努利试验成功，则伯努利随机变量取值为1。若伯努利试验失败，则伯努利随机变量取值为0。记其成功概率为 <img src="https://latex.codecogs.com/png.latex?  {\displaystyle p(0{\leq }p{\leq }1)} ">，失败概率为<img src="https://latex.codecogs.com/png.latex?  {\displaystyle q=1-p} ">。则

- 其概率质量函数为：
<img src="https://latex.codecogs.com/png.latex?  {\displaystyle f_{X}(x)=p^{x}(1-p)^{1-x}.} ">
- 其期望值为：
<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \operatorname {E} [X]=\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}  ">
- 其方差为：
<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \operatorname {var} [X]=\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq} ">

在介绍接下来的内容之前，我们先了解一下似然函数的意义。

    概率，用于在已知一些参数的情况下，预测接下来在预测上所得到的结果；
    似然性，则是用于在已知某些预测所得到的结果时，对有关事物之性质的参数进行估计

在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数B时，事件A会发生的概率写作：

<img src="https://latex.codecogs.com/png.latex?  P(A\,|\,B)={\frac{P(A,B)}{P(B)}} ">

利用贝叶斯定理，

<img src="https://latex.codecogs.com/png.latex?  P(B\,|\,A)={\frac  {P(A\,|\,B)\;P(B)}{P(A)}}">

因此，我们可以反过来构造表示似然性的方法：已知有事件A发生，运用似然函数 <img src="https://latex.codecogs.com/png.latex?  {\mathbb  {L}}(B\,|\,A)">，我们估计参数B的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了：

<img src="https://latex.codecogs.com/png.latex?  b\mapsto P(A\,|\,B=b)\!">

注意到这里并不要求似然函数满足归一性：<img src="https://latex.codecogs.com/png.latex?  \sum _{{b\in {\mathcal  {B}}}}P(A\,|\,B=b)=1">。一个似然函数乘以一个正的常数之后仍然是似然函数。对所有<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \alpha >0} ">，都可以有似然函数：

<img src="https://latex.codecogs.com/png.latex?  L(b\,|\,A)=\alpha \;P(A\,|\,B=b)\!">

现在假设我们有一个数据集<img src="https://latex.codecogs.com/png.latex?  D={x_1,\ x_2,\ ...,\ x_N}">独立同分布，那么该数据集的似然函数是：<img src="https://latex.codecogs.com/png.latex?  p(D\,|\,\mu)=\prod_{n=1}^Np(x_n,\mu)=\prod_{n=1}^N\mu^{x_n}(1-\mu)^{1-x_n} ">

此时，该似然函数的意义是，以<img src="https://latex.codecogs.com/png.latex?  \mu">为自变量，实验时收集到数据集D的可能性的大小（在归一化条件下，就是收集到数据集D的概率），也就是说，对于不同的<img src="https://latex.codecogs.com/png.latex?  \mu">，收集到数据集D的概率是不同的，那么最大似然估计就是在该似然函数取得最大值时<img src="https://latex.codecogs.com/png.latex?  \mu">的取值。

为了方便求导，这里取对数似然函数的对<img src="https://latex.codecogs.com/png.latex?  \mu">的导数

<img src="https://latex.codecogs.com/png.latex?  \frac{d\,ln\,p(D\,|\,\mu)}{d\,\mu}=0 ">

解得：

<img src="https://latex.codecogs.com/png.latex?  \mu_{ML}=\frac1N\sum_{n=1}^Nx_n ">

### B分布

Β分布也称贝塔分布（Beta distribution），是指一组定义在<img src="https://latex.codecogs.com/png.latex?  {\displaystyle (0,1)} ">区间的连续概率分布，有两个参数 <img src="https://latex.codecogs.com/png.latex?  {\displaystyle \alpha ,\beta >0} ">。

Β分布的概率密度函数是：

<img src="https://latex.codecogs.com/png.latex?  f(x;\alpha,\beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du} = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1}">

Beta函数：

<img src="https://latex.codecogs.com/png.latex? \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}={\int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du}">

归一化：

<img src="https://latex.codecogs.com/png.latex? {\int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du}=1">

其均值和方差分别为

<img src="https://latex.codecogs.com/png.latex?  \mu =\operatorname {E}(X)={\frac  {\alpha }{\alpha +\beta }} ">

<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \operatorname {Var} (X)=\operatorname {E} (X-\mu )^{2}={\frac {\alpha \beta }{(\alpha +\beta )^{2}(\alpha +\beta +1)}}} ">

在上式中，我们还需介绍一个函数，伽马函数。

在数学中，<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \Gamma \,} ">函数，也叫做伽玛函数（Gamma函数），是阶乘函数在实数与复数域上的扩展。如果n为正整数，则：

<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \Gamma (n)=(n-1)!} ">

对于实数部分为正的复数<img src="https://latex.codecogs.com/png.latex? z">，伽玛函数定义为：

<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \Gamma (z)=\int _{0}^{\infty }{\frac {t^{z-1}}{\mathrm {e} ^{t}}}\,{\rm {d}}t} ">

值得注意的是，因为在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。我们发现上述先验函数和B分布的概率密度函数的函数形式相似，所以B分布的概率密度函数与该先验函数具有共轭性。

### 贝叶斯概率

先验概率分布：  <img src="https://latex.codecogs.com/png.latex? p(w)">

观测数据：     <img src="https://latex.codecogs.com/png.latex? D=\{t_1,t_2,...,t_N\}">

似然函数：     <img src="https://latex.codecogs.com/png.latex? p(D|w)">

朴素贝叶斯模型：<img src="https://latex.codecogs.com/png.latex? P(A|B)=\frac{p(A|B)p(A)}{p(B)}">

后验概率:      <img src="https://latex.codecogs.com/png.latex? P(w|D)=\frac{p(D|w)p(w)}{p(D)}">

后验概率与似然函数和先验概率的乘积成正比

由上知，后验概率与先验和似然的乘积成正比，所以令后验概率=似然函数X先验概率

<img src="https://latex.codecogs.com/png.latex? p(\mu|a,b,D)=(\prod ^N_{n=1}\mu^{x_n}(1-\mu)^{1-x_n})(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1})=p(\mu|m,l,a,b)">

其中，<img src="https://latex.codecogs.com/png.latex? m=\sum_{n=1}^Nx_n,\ l=N-m">

事实上，这是另一个Bata分布，其均值和方差分别为

<img src="https://latex.codecogs.com/png.latex? \operatorname {E}(\mu)={\frac  {a}{a+b}} ">

<img src="https://latex.codecogs.com/png.latex?  {\displaystyle \operatorname {Var} (x=1|D)=\frac{m+a}{m+a+l+b} }">

a和b分别为x=1和x=0的观测数（不一定为整数），该式子的意义是：每次取一个观测值，然后在每次观测值后更新当前的后验分布（更新方法是让当前的先验分布与新观测值的似然函数相乘，然后归一化，获得新的修正后的后验分布，即将每次得到的后验分布作为下一次观测到的先验分布）。

此方法为顺序方法学习，可以被用于实时学习场景中，在实时学习的场景中，输入一个稳定持续的数据流，模型必须在观测所有数据之前就进行观测。

贝叶斯的结果会和最大似然的结果在N-><img src="https://latex.codecogs.com/png.latex? \infty">会统一到一起，对于有限规模的数据集，\mu的后验均值总是位于先验均值和最大似然估计之间，随着观测数据增多，后验概率的不确定性会持续下降。

## 多元变量

对于多元变量，一种方便的表示方法就是“1-of-k”表示法，这种表示法中，变量被表示成一个K维向量x，向量中的一个元素<img src="https://latex.codecogs.com/png.latex? x_k=1">，剩余元素等于0。

<img src="https://latex.codecogs.com/png.latex? X=(0,0,1,0,0,0)^T">

<img src="https://latex.codecogs.com/png.latex? \mu=(\mu_1,...,\mu_k)^T\ \ \ \ \mu_k\geqslant 0">

当k=2时，<img src="https://latex.codecogs.com/png.latex?Bern(x\mu)=\mu^x(1-\mu)^{1-x}">

给定数据集<img src="https://latex.codecogs.com/png.latex?D=\{x_1,...,x_N\}">

似然函数：

<img src="https://latex.codecogs.com/png.latex?p(D|\mu)=\prod _{n=1}^N\prod ^K_{k=1}\mu_k^{x_{nk}}=\prod ^K_{k=1}\mu_k^{\sum _nx_{nk}}=\prod _{k=1}^K\mu_k^{m_k}">

其中充分统计量：<img src="https://latex.codecogs.com/png.latex?m_k=\sum_nx_{nk}">

为了找到<img src="https://latex.codecogs.com/png.latex?\mu">的最大似然解，我们需要关于<img src="https://latex.codecogs.com/png.latex?\mu_k">最大化<img src="https://latex.codecogs.com/png.latex?ln p(D|\mu)">，并且要限制<img src="https://latex.codecogs.com/png.latex?\mu_k">的和必须等于1.这可以通过拉格朗日乘数法来实现，即最大化

<img src="https://latex.codecogs.com/png.latex?\sum _{k=1}^Km_kln\,\mu_k+\lambda (\sum_{k=1}^K\mu_k-1)">

解得<img src="https://latex.codecogs.com/png.latex?\mu_k^{ML}=\frac{m_k}N">

所以，

<img src="https://latex.codecogs.com/png.latex?Mult(m_1,...,m_k|\mu,N)=\begin{pmatrix}&space;N\\&space;m_1m_2...m_k&space;\end{pmatrix}\prod&space;^K_{k=1}\mu_k^{m_k}">

其中

<img src="https://latex.codecogs.com/png.latex?\begin{pmatrix}&space;N\\&space;m_1m_2...m_k&space;\end{pmatrix}=\frac{N!}{m_1!m_2!...m_k!}">

<img src="https://latex.codecogs.com/png.latex?\sum_{k=1}^Km_k=N">

### Dirichlet分布

多项式分布的共轭先验为

<img src="https://latex.codecogs.com/png.latex?p(\mu|\alpha)\propto \prod^K_{k=1}\mu_k^{m_k}">

概率的归一化形式，即狄利克雷分布为：

<img src="https://latex.codecogs.com/png.latex?Dir(\mu|\alpha)=\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)...\Gamma(\alpha_K)}\prod^K_{k=1}\mu_k^{m_k}">

## 指数族分布

指数族分为离散型和连续型，其中离散型分为二元变量（伯努利分布，二项分布，Beta分布）和多元变量（多项式分布，狄利克雷分布），连续型分为高斯分布、伽马分布、学生t分布和冯·米塞斯分布。这类分布有这样的形式

<img src="https://latex.codecogs.com/png.latex?p(x|\eta)=h(x)g(\eta)exp\{\eta^Tu(x)\}">

其中，x可能是标量或是向量，可能是离散的或是连续的。<img src="https://latex.codecogs.com/png.latex?\eta">是概率分布的自然参数，u(x)是x的某个函数，函数g(x)可看作系数，它确保了概率分布是归一化的，因此

<img src="https://latex.codecogs.com/png.latex?g(\eta)\int h(x)exp\{\eta^Tu(x)\}dx=1">

对于伯努利分布而言

<img src="https://latex.codecogs.com/png.latex?h(x)=1,\ g(\eta)=1-\mu,\ \eta=ln(\frac\mu{1-\mu}),\ u(x)=x">

并且由<img src="https://latex.codecogs.com/png.latex?\eta=ln(\frac\mu{1-\mu})">可以解出<img src="https://latex.codecogs.com/png.latex?\mu=\sigma (\eta)=\frac1{1+exp(-\eta)}">，被称作logistic sigmoid函数

对于多项式分布而言

<img src="https://latex.codecogs.com/png.latex?h(x)=1,\ g(\eta)=1,\ \eta=(\eta_1,...,\eta_M)^T,\ (\eta_k=ln\,\mu_k)">

<img src="https://latex.codecogs.com/png.latex?u(x)=x\ (x=(x_1,...,x_M))">

但是<img src="https://latex.codecogs.com/png.latex?\eta_k">并不独立，因为参数<img src="https://latex.codecogs.com/png.latex?\mu_k">有限制，<img src="https://latex.codecogs.com/png.latex?\sum_{k=1}^M\mu_k=1">，带入原式得到：

<img src="https://latex.codecogs.com/png.latex?exp\{\sum_{k=1}^Mx_k\ln\,\mu_k\}=exp\{\sum_{k=1}^{M-1}x_k\ln\,\mu_k+(1-\sum_{k=1}^{M-1}x_k)\ln\,(1-\sum_{k=1}^{M-1}\mu_k)\}">

<img src="https://latex.codecogs.com/png.latex? =exp\{\sum_{k-1}^{M-1}x_k\ln\,\frac{\mu_k}{1-\sum_{j=1}^{M-1}\mu_j}+\ln(1-\sum_{k=1}^{M-1}\mu_k)\}">

所以得

<img src="https://latex.codecogs.com/png.latex?\eta_k=ln\,\frac{\mu_k}{1-\sum_{j=1}^{M-1}\mu_j}\Rightarrow \mu_k=\frac{e^{\eta_k}}{1+\sum_{j=1}^{M-1}e^{\eta_j}}">

被称为softmax函数（或归一化指数）

对于高斯分布而言

<img src="https://latex.codecogs.com/png.latex?\eta=\begin{pmatrix}&space;\mu/\sigma^2\\&space;-1/2\sigma^2&space;\end{pmatrix},\,\,\,\,u(x)=\begin{pmatrix}&space;x\\&space;x^2&space;\end{pmatrix},\,\,\,\,h(x)=(2\pi)^{-\frac12},\,\,\,\,g(\eta)=(-2\eta_2)^{\frac12}exp(\frac{\eta_1^2}{4\eta_2})">

### 最大似然和充分统计量

假设<img src="https://latex.codecogs.com/png.latex?X={x_1,...,x_n}">，则其似然函数如下

<img src="https://latex.codecogs.com/png.latex?p(X\mid\mu)=(\prod _{n=1}^N)h(x_n)g(\eta)^Nexp\{\eta^T\sum_{n=1}^Nu(x_n)\}">

对对数似然函数求<img src="https://latex.codecogs.com/png.latex?\eta">的偏导得到

<img src="https://latex.codecogs.com/png.latex?-\bigtriangledown \ln\,g(\eta_{ML})=\frac1N\sum_{n=1}^Nu(x_n)">

上式即被称为充分估计量

伯努利分布的充分估计量为<img src="https://latex.codecogs.com/png.latex?u(x)=x">

高斯分布的充分估计量为<img src="https://latex.codecogs.com/png.latex?u(x)=(x,x^2)^T">

### 共轭先验

指数族函数

<img src="https://latex.codecogs.com/png.latex?p(x\mid\eta)=h(x)g(\eta)exp\{\eta^Tu(x)\}">

似然函数

<img src="https://latex.codecogs.com/png.latex?p(X\mid\eta)=(\prod_{n=1}^Nh(x_n))g(\eta)^Nexp\{\eta^T\sum_{n=1}^Nu(x_n)\}">

共轭先验

<img src="https://latex.codecogs.com/png.latex?p(\eta\mid\chi ,\nu)=f(\chi ,\nu )g(\eta)^{\nu }exp\{\nu \eta^T\chi \}">

其中，<img src="https://latex.codecogs.com/png.latex?f(\chi,\nu)">为归一化系数

后验概率

<img src="https://latex.codecogs.com/png.latex?p(\eta\mid X,\chi,\nu)\propto g(\eta)^{\nu+N}exp\{\eta^T(\sum_{n=1}^Nu(x_n)+\nu\chi)\}">

### 无信息先验

在对分布应该具有的形式几乎完全不知道时，可寻找一种形式的先验分布（令先验分布为均匀分布）

困难一：对于由参数控制的分布<img src="https://latex.codecogs.com/png.latex?p(x\mid\lambda)">，若<img src="https://latex.codecogs.com/png.latex?\lambda">无界，则先验分布无法归一化，因为对<img src="https://latex.codecogs.com/png.latex?\lambda">的积分是发散的，这样的先验分布是反常的。

困难二：若概率发生非线性变量的概率密度的变换，例如

<img src="https://latex.codecogs.com/png.latex?P_{\lambda}(\lambda)=c,\ \ \because \lambda =\eta^2\ \ \therefore P_{\eta}(\eta)=P_{\lambda}(\lambda)\left | \frac{d\lambda}{d\eta} \right |=2P_{\lambda}(\eta^2)\eta">

结果不再为常数

## 无参数方法

概率密度估计的直方图方法：

1. 为估计在某个特定位置的概率密度，应考虑位于那个点的
某个领域内的数据点
2. 为获得好的结果，平滑参数的值既不能太大也不能太小

### 核密度估计

假设有一未知的概率密度分布P(x)，收集了N次观测，每个数据点落入R中的概率为

<img src="https://latex.codecogs.com/png.latex?Bin(K,N,P)=\frac{N!}{K!(N-K)!}P^K(1-P)^{N-K}">

其中<img src="https://latex.codecogs.com/png.latex?P=\int_Rp(x)dx">

如果N较大，则<img src="https://latex.codecogs.com/png.latex?P\approx \frac KN">，即<img src="https://latex.codecogs.com/png.latex?K\approx NP">

如果R足够小，使在这个区域的概率密度P(x)大致为常数则<img src="https://latex.codecogs.com/png.latex?P\approx p(x)V">

其中V为区域R的体积

由上述两个假设得到的式子得：<img src="https://latex.codecogs.com/png.latex?p(x)=\frack{NV}">

此式依赖于两个相互矛盾的假设，即区域R要足够小，使这个区域内的概率密度函数近似为常数，但也要足够大，使K足够让二项分布达到尖峰。

核方法：固定V然后从数据中确定K

R取为以x为中心的D维小方体，定义<img src="https://latex.codecogs.com/png.latex?k(u)=\left\{\begin{matrix}&space;1\,\,|u_i|\leqslant&space;\frac12\,\,i=1..D\\&space;0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,else\,\,\,\,\,\,\,\,\,\,\,\,\,\,&space;\end{matrix}\right.">

所以当边长为h时，<img src="https://latex.codecogs.com/png.latex?k(\frac{x-x_n}n)=\left\{\begin{matrix}&space;1\,\,\,\,in\\&space;0\,\,\,out&space;\end{matrix}\right.">，所以位于方体内数据点的总数为<img src="https://latex.codecogs.com/png.latex?K=\sum_{n=1}^Nk(\frac{x-x_n}n)">，因为D维边长为h的方体V=h^D，所以核密度估计或Parzen估计为

<img src="https://latex.codecogs.com/png.latex?P(x)=\frac1N\sum_{n=1}^N\frac1{h^D}k(\frac{x-x_n}{n})">

这种方法存在问题：人为带来的不连续性，且训练阶段只需要存储数据。

### 最近邻方法

一个以x为中心的饿小球体且半径可以自由增长，直到他精确的包含k个数据点，利用公式<img src="https://latex.codecogs.com/png.latex?P(x)=\fracK{NV}">，计算概率分布。近邻方法是固定K的值然后确定V，可以推广到分类。

给定一个数据集，其中<img src="https://latex.codecogs.com/png.latex?N_k">个数据属于类别<img src="https://latex.codecogs.com/png.latex?C_k">，数据点总数为N，所以<img src="https://latex.codecogs.com/png.latex?\sum{N_k}=N">。现对一新的数据点进行分类，画一个以x为中心的球且精确地包含K个数据点（无论属于哪一个类别），设球体体积为V，且包含<img src="https://latex.codecogs.com/png.latex?C_k">类得<img src="https://latex.codecogs.com/png.latex?K_k">个数据点，则此类的似然函数（即每个类别关联的概率密度估计）为

<img src="https://latex.codecogs.com/png.latex?p(x\mid C_k)=\frac{K_k}{NV}">

先验概率

<img src="https://latex.codecogs.com/png.latex?p(C_k)=\frac{N_k}N">

由贝叶斯定理得

<img src="https://latex.codecogs.com/png.latex?p(C_k\mid x)=\frac{p(x\mid C_k)p(C_k)}p(x)=\frac{K_k}K">

因为要最小化错误分类的概率，所以应该把测试点x分配给有最大后验概率的类别，对应最大的<img src="https://latex.codecogs.com/png.latex?\frac{K_k}K">

最近邻(k=1)分类器在<img src="https://latex.codecogs.com/png.latex?N \rightarrow \infty">时，错误率不会超过最优分类器（即用真实概率分布的分类器）可达到的最小错误率的2倍
