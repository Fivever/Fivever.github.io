---
layout: post
title: 概率分布
date: 2019-03-10
categories: 模式识别与机器学习
tag: 读书笔记
published: true
---

* content
{:toc}

# 概率分布

<img src="http://chart.googleapis.com/chart?cht=tx&chl= ">

## 二元变量

伯努利分布（英语：Bernoulli distribution，又名两点分布或者0-1分布，是一个离散型概率分布，为纪念瑞士科学家雅各布·伯努利而命名。）若伯努利试验成功，则伯努利随机变量取值为1。若伯努利试验失败，则伯努利随机变量取值为0。记其成功概率为 <img src="http://chart.googleapis.com/chart?cht=tx&chl= {\displaystyle p(0{\leq }p{\leq }1)} ">，失败概率为<img src="http://chart.googleapis.com/chart?cht=tx&chl= {\displaystyle q=1-p} ">。则

- 其概率质量函数为：
<img src="http://chart.googleapis.com/chart?cht=tx&chl= {\displaystyle f_{X}(x)=p^{x}(1-p)^{1-x}.} ">
- 其期望值为：
<img src="http://chart.googleapis.com/chart?cht=tx&chl= {\displaystyle \operatorname {E} [X]=\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}  ">
- 其方差为：
<img src="http://chart.googleapis.com/chart?cht=tx&chl= {\displaystyle \operatorname {var} [X]=\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq} ">

在介绍接下来的内容之前，我们先了解一下似然函数的意义。

	概率，用于在已知一些参数的情况下，预测接下来在预测上所得到的结果；
    似然性，则是用于在已知某些预测所得到的结果时，对有关事物之性质的参数进行估计

在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数B时，事件A会发生的概率写作：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= P(A\mid B)={\frac{P(A,B)}{P(B)}} ">

利用贝叶斯定理，

<img src="http://chart.googleapis.com/chart?cht=tx&chl= P(B\mid A)={\frac  {P(A\mid B)\;P(B)}{P(A)}}">

因此，我们可以反过来构造表示似然性的方法：已知有事件A发生，运用似然函数 <img src="http://chart.googleapis.com/chart?cht=tx&chl= {\mathbb  {L}}(B\mid A)">，我们估计参数B的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= b\mapsto P(A\mid B=b)\!">

注意到这里并不要求似然函数满足归一性：<img src="http://chart.googleapis.com/chart?cht=tx&chl= \sum _{{b\in {\mathcal  {B}}}}P(A\,|\,B=b)=1">。一个似然函数乘以一个正的常数之后仍然是似然函数。对所有<img src="http://chart.googleapis.com/chart?cht=tx&chl= {\displaystyle \alpha >0} ">，都可以有似然函数：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= L(b\,|\,A)=\alpha \;P(A\,|\,B=b)\!">

现在假设我们有一个数据集<img src="http://chart.googleapis.com/chart?cht=tx&chl= D={x_1,\ x_2,\ ...,\ x_N}">独立同分布，那么该数据集的似然函数是：<img src="http://chart.googleapis.com/chart?cht=tx&chl= p(D\,|\,\mu)=\prod_{n=1}^Np(x_n,\mu)=\prod_{n=1}^N\mu^{x_n}(1-\mu)^{1-x_n} ">

此时，该似然函数的意义是，以<img src="http://chart.googleapis.com/chart?cht=tx&chl= \mu">为自变量，实验时收集到数据集D的可能性的大小（在归一化条件下，就是收集到数据集D的概率），也就是说，对于不同的<img src="http://chart.googleapis.com/chart?cht=tx&chl= \mu">，收集到数据集D的概率是不同的，那么最大似然估计就是在该似然函数取得最大值时<img src="http://chart.googleapis.com/chart?cht=tx&chl= \mu">的取值。

为了方便求导，这里取对数似然函数的对<img src="http://chart.googleapis.com/chart?cht=tx&chl= \mu">的导数<img src="http://chart.googleapis.com/chart?cht=tx&chl= \frac{d\,ln\,p(D\,|\,\mu)}{d\,\mu}=0 ">，解得：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= \mu_{ML}=\frac1N\sum_{n=1}^Nx_n ">
